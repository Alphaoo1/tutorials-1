{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Machine Learning (ML) using Tensorflow\n",
    "\n",
    "References: \n",
    "\n",
    "* [Tensorflow Tutorial](https://www.tensorflow.org/versions/master/get_started/mnist/beginners)\n",
    "\n",
    "MNIST is a simple computer vision dataset. It consists of images of handwritten digits like these:\n",
    "\n",
    "![](https://www.tensorflow.org/images/MNIST.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"A very simple MNIST classifier.\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/beginners\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import \n",
    "\n",
    "The MNIST data is hosted on [this website](http://yann.lecun.com/exdb/mnist/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts: \n",
    "\n",
    "1. 55,000 data points of training data (`mnist.train`), \n",
    "2. 10,000 points of test data (`mnist.test`), \n",
    "3. 5,000 points of validation data (`mnist.validation`). \n",
    "\n",
    "This split is very important: it's of course essential in ML that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!\n",
    "\n",
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "Thus after flattening the image into vectors of 28*28=784, we obtain as `mnist.train.images` a tensor (an n-dimensional array) with a shape of [55000, 784]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "MNIST images is of a handwritten digit between zero and nine. So there are only ten possible things that a given image can be. \n",
    "We want to be able to look at an image and give the probabilities for it being each digit, thus base on Softmax Regressions as activation function. `softmax()` has the advantage of allowing for an easy mapping to a probability (as sum = 1) and thus can be used a nice last layout of the ML process. \n",
    "\n",
    "* See also [List of activation function](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)\n",
    "\n",
    "A softmax regression has two steps: \n",
    "\n",
    "1. first we add up the evidence of our input image being in certain classes. For that, we do a weighted sum of the pixel intensities $y=W*x+b$, where the weight is negative if that pixel having a high intensity is evidence against the image being in that class, and positive if it is evidence in favor. \n",
    "2. and then we convert that evidence into probabilities throught the application of the `softmax()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "x  = tf.placeholder(tf.float32, [None, 784])  # Placeholder for the input images\n",
    "W  = tf.Variable(tf.zeros([784, 10]))         # Model paramerer: weight\n",
    "b  = tf.Variable(tf.zeros([10]))              # Model parameter; bias\n",
    "z  = tf.matmul(x, W) + b                      # BEFORE applying softmax\n",
    "# Real model: \n",
    "# y = tf.nn.softmax(z)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])   # Placeholder to input the **correct** answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "So our expected model is provided by the following formula:\n",
    "\n",
    "$$y = softmax(W*x+b)$$\n",
    "\n",
    "In order to train this model, we need to define what it means for a model to be bad thought a _loss_ function expected to be minimized. One very common, very nice function to determine the loss of a model is called \"cross-entropy\", defined as:\n",
    "\n",
    "$$H_{y'}(y) = -\\sum_i y'_i \\log(y_i) = -\\sum y' \\log(y)$$\n",
    "\n",
    "where $y$ is our predicted probability distribution, and $yâ€²$ is the true distribution (i.e. `y_` in the above model definition).\n",
    "Since this raw formulation can be numerically unstable, we will apply instead `tf.nn.softmax_cross_entropy_with_logits` on the raw outputs of 'y', and then average across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The raw formulation of cross-entropy,\n",
    "#\n",
    "#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(z)),\n",
    "#                                 reduction_indices=[1]))\n",
    "#\n",
    "# can be numerically unstable.\n",
    "#\n",
    "# So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "# outputs of 'y', and then average across the batch.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we know what we want our model to do, it's very easy to have TensorFlow train it to do so. \n",
    "Because TensorFlow knows the entire graph of your computations, it can automatically use the [backpropagation algorithm](https://colah.github.io/posts/2015-08-Backprop) to efficiently determine how the variables $W$ and $b$  affect the loss to be minimized. \n",
    "Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss.\n",
    "\n",
    "Tensorflow offers a broad range of [optimization algorithms](https://www.tensorflow.org/versions/master/api_guides/python/train#Optimizers) for the training, here we are going to minimize cross_entropy using the [gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent) with a learning rate of 0.5. Gradient descent is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, Tensorflow actually add new operations to the graph of the model which implement backpropagation and gradient descent.\n",
    "\n",
    "## Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run() # initialize the variables created (W and b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train - we'll run the training step 1000 times.\n",
    "Each step of the loop, we get a \"batch\" of one hundred random data points from our training set. We run train_step feeding in the batches data to replace the placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train - we'll run the training step 1000 times!\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Let's figure out where we predicted the correct label. It is of course **crucial** to use another set of images from `mnist.test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(z, 1), tf.argmax(y_, 1))   # return a list of booleans\n",
    "# To determine what fraction are correct, we cast to floating point numbers and then take the mean. \n",
    "# For example, [True, False, True, True] would become [1,0,1,1] which would become 0.75.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                    y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means and accuracy of around 92%, when the [best models](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results) allow for 99.79% of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
