
# Accelerating Applications with CUDA C/C++

## CUDA

Objectives
By the time you complete this lab, you will be able to:

Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.
Control parallel thread hierarchy using execution configuration.
Refactor serial loops to execute their iterations in parallel on a GPU.
Allocate and free memory available to both CPUs and GPUs.
Handle errors generated by CUDA code.
Accelerate CPU-only applications.

## Access to a GPU node

Reserve a node with one GPU, load the necessary modules, and save them for a quick restore.

```bash
> srun -n1 -c1 --gres=gpu:1 -pgpu --pty bash -i
$ module av cuda compiler/gcc
$ module load compiler/LLVM system/CUDA
$ nvidia-smi
$ module save cuda
```

In fact, you can compile CUDA applications on a node without GPU, using the same modules.
Only the code will not execute properly.

## Writing Application Code for the GPU

CUDA provides extensions for many common programming languages, in the case of this tutorial, C/C++. 
There are several API available for GPU programming, with either specialization, or abstraction.
The main API is the *CUDA Runtime*. 
The other, lower level, is the *CUDA Driver*, which also offers more customization options.
Other APIs are Thrust, NCCL.

### Hello World

Below is a partial `.cu` program (`.cu` is the file extension for CUDA-accelerated programs). 
It contains two functions, the first which will run on the CPU, the second which will run on the GPU. 
Spend a little time identifying the differences between the functions, both in terms of how they are defined, and how they are invoked.

```
#include "cuda.h"

void CPUFunction()
{
  printf("Runs on the CPU.\n");
}

__global__ 
void GPUFunction()
{
  printf("Runs on the GPU.\n");
}

int main()
{
  CPUFunction();

  GPUFunction<<<1, 1>>>();

  cudaDeviceSynchronize();

  return EXIT_SUCCESS;
}
```
Here are some important lines to highlight, as well as some other common terms used in accelerated computing:
```
__global__
void GPUFunction()
```

The `__global__` keyword indicates that the following function will run on the GPU, and can be invoked globally, which means either by the CPU or GPU.
Often, code executed on the CPU is referred to as host code, and code running on the GPU is referred to as device code.
Notice the return type `void`. 
It is required that functions defined with the `__global__` keyword return type void.
```
GPUFunction<<<1, 1>>>();
```

Typically, when calling a function to run on the GPU, we call this function a kernel, which is launched.
When launching a kernel, we must provide an execution configuration, which is done by using the `<<< ... >>>` syntax just prior to passing the kernel any expected arguments.
At a high level, execution configuration allows programmers to specify the thread hierarchy for a kernel launch, which defines the number of thread groupings (called blocks), as well as how many threads to execute in each block.

The execution configuration allows programmers to specify details about launching the kernel to run in parallel on multiple GPU threads.
More precisely, the execution configuration allows programmers to specifiy how many groups of threads - called thread blocks, or just blocks - and how many threads they would like each thread block to contain.
The syntax for this is:
```
<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>
```
The kernel code is executed by every thread in every thread block configured when the kernel is launched.

```
cudaDeviceSynchronize();
```
Unlike much C/C++ code, launching kernels is asynchronous: the CPU code will continue to execute without waiting for the kernel launch to complete.
A call to ``cudaDeviceSynchronize`, a function provided by the CUDA runtime, will cause the host (CPU) code to wait until the device (GPU) code completes, and only then resume execution on the CPU.

### Compiling and Running Accelerated CUDA Code
This section contains details about the *nvcc* command you issue to compile and run your `.cu` program.

The CUDA platform ships with the NVIDIA CUDA Compiler `nvcc`, which can compile CUDA accelerated applications, both the host, and the device code they contain. 
After completing the lab, For anyone interested in a deeper dive into `nvcc`, start with the documentation (`nvcc --help`).

Compiling and executing `some-CUDA.cu` source file:
```bash
$ nvcc -arch=sm_70 -o out some-CUDA.cu -run
```

`nvcc` is the command line command for using the nvcc compiler.
`some-CUDA.cu` is passed as the source file to compile.
The `o` flag is used to specify the output file for the compiled program. 
The `arch` flag indicates for which architecture the files must be compiled. 
For the present case `sm_70` will serve to compile specifically for the Volta GPUs.
This will be further explained in a following section. 
As a matter of convenience, providing the `run` flag will execute the successfully compiled binary.

`nvcc` parses the C++ language (it used to be C).

### Exercise: Launch Parallel Kernels

The following program currently makes a very basic function call that prints a message.

```
/*
 * FIXME
 */

void helloCPU()
{
  std::cout<<"Hello from CPU.\n";
}

void helloGPU()
{
  printf("Hello also from GPU.\n");
}

int main()
{

  helloCPU();
  helloGPU();
  
  return EXIT_SUCCESS;
}

```

Fix and refactor the code such that `helloGPU` kernel to execute in parallel on 5 threads, all executing in a single thread block.
You should see the output message printed 5 times after compiling and running the code.

Refactor the `helloGPU` kernel again, this time to execute in parallel inside 5 thread blocks, each containing 5 threads.
You should see the output message printed 25 times now after compiling and running.

### Compiling and Running Accelerated CUDA Code, part 2
The following compilation command works:
```bash
$ nvcc -o out some-CUDA.cu
```
However, there is problem with this code.
Cuda uses a two stage compilation process, to PTX, and to binary.

To produce the PTX for the cuda kernel, use:
```bash
$ nvcc -ptx -o out.ptx some-CUDA.cu  # -o is optional
```
Brief inspection of the generated PTX file reports a *target* real platform of `sm_30`.
```
// Based on LLVM 3.4svn
//

.version 6.3
.target sm_30
.address_size 64
```
The Volta GPU implements `sm_70` or `sm_72`. So, we could be missing some features from the GPU.

To specify an instruction set, use the `-arch` option:
```bash
$ nvcc -o out.ptx -ptx -arch=compute_70 some-CUDA.cu
```
To produce an executable, instead of just the PTX code, and specify an instruction set, use the `-arch` option, for example:
```bash
$ nvcc -o out -arch=compute_70 some-CUDA.cu
```
This actually produces an executable that embeds the kernels' code as PTX, of the specified instruction set. 
The PTX code will then be JIT compiled when executed, matching the real GPU instruction set. 
To see this, search for the target PTX instruction in the executable:
```bash
$ strings out | grep target
```

The `code` option specifies what the executable contains.
The following nvcc options specify that the executables contains the binary code for the real GPU `sm_70`.
```bash
$ nvcc -o out -arch=compute_70 -code=sm_70 some-CUDA.cu
```
The following nvcc options specify that the executables contains the binary code for the real GPU `sm_70`, *and* the PTX code for the `sm_70`.
```bash
$ nvcc -o out -arch=compute_70 -code=sm_70,compute_70 some-CUDA.cu
```
To observe the difference, search for the target PTX command, in both commands:
```bash
$ strings out | grep target
```
Actually, the first compilation instruction:
```bash
$ nvcc -o out -arch=sm_70 some-CUDA.cu
```
is a shorthand for the full command:
```bash
$ nvcc -o out -arch=compute_70 -code=sm_70,compute_70 some-CUDA.cu
```
You may want to package the PTX to allow for JIT compilation across different real GPU.
```bash
$ nvcc -o out -arch=compute_70 some-CUDA.cu
```

## CUDA Thread Hierarchy

Each thread will execute the same kernel function.
Therefore, some mechanism must be available for each thread to work on different memory location.
A key CUDA mechanism for this coordination involves builtin constants, that can be used to uniquely identify threads of a launch configuration.
Other aspects involved thread synchronization (not covered here besides the host-side synchronization function).

### Thread and Block Indices
Each thread is given an index within its thread block, starting at 0. 
Additionally, each block is given an index, starting at 0. 
Just as threads are grouped into thread blocks, blocks are grouped into a grid, which is the highest entity in the CUDA thread hierarchy. 
In summary, CUDA kernels are executed in a grid of 1 or more blocks, with each block containing the same number of 1 or more threads.

CUDA kernels have access to special variables identifying both the index of the thread (within the block) that is executing the kernel, and, the index of the block (within the grid) that the thread is within. 
These variables are `threadIdx.x` and `blockIdx.x` respectively.

The `.x` suggests that there more dimensions to these variables, they can be up to 3 dimensions.
We will only see examples with one dimension here. 
Refer to the (programming guide)[https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy] for more information.

Within a block, the thread ID is the same as the threadIdx.x. 
There is a maximum of 1024 threads allowed per block. 
Within a grid (full configuration), the block ID is the same as the blockIdx.x.
Together, the number of blocks and number of threads allow to exceed the 1024 threads limit.

### Exercise: Use Specific Thread and Block Indices

The program below contains a working kernel that is not printing a success message. 
Edit the source code to update the execution configuration so that the success message will print. 
```
/*
 * FIXME
 */

#include <cstdio>

void printSuccessForCorrectExecutionConfiguration()
{
  if (threadIdx.x == 1023 && blockIdx.x == 255)
  {
    printf("Success!\n");
  }
}

int main()
{
  /*
   * Update the execution configuration so that the kernel
   * will print `"Success!"`.
   */

  printSuccessForCorrectExecutionConfiguration<<<1, 1>>>();
}
```

### Accelerating For Loops

For loops in CPU-only applications are ripe for acceleration: rather than run each iteration of the loop serially, each iteration of the loop can be run in parallel in its own thread. 
Consider the following for loop, and notice, though it is obvious, that it controls how many times the loop will execute, as well as defining what will happen for each iteration of the loop:

```
int N = 2<<10;
for (int i = 0; i < N; ++i)
{
  printf("%d\n", i);
}
```
In order to parallelize this loop, 2 steps must be taken:

A kernel must be written to do the work of a single iteration of the loop. 
Because the kernel will ignore other running kernels, the execution configuration must be such that the kernel executes the correct number of times, for example, the number of times the loop would have iterated.

### Exercise: Accelerating a For Loop with a Single Block of Threads

Currently, the loop function runs a for loop that will serially print the numbers 0 through 9. 
Modify the loop function to be a CUDA kernel which will launch to execute N iterations in parallel. 
After successfully refactoring, the numbers 0 through 9 should still be printed.
```
/*
 * Refactor `loop` to be a CUDA Kernel. 
 * The new kernel should only do the work 
 * of 1 iteration of the original loop.
 */

#include <cstdio>

void loop(int N)
{
  for (int i = 0; i < N; ++i)
  {
    printf("This is iteration number %d\n", i);
  }
}

int main()
{
  /*
   * When refactoring `loop` to launch as a kernel, be sure
   * to use the execution configuration to control how many
   * "iterations" to perform.
   *
   * Use 1 block of threads.
   */

  int N = 10;
  loop(N);
}
```

### Using Block Dimensions for More Parallelization

CUDA Kernels have access to another special variable that gives the number of threads in a block: blockDim.x. 
Using this variable, in conjunction with blockIdx.x and threadIdx.x, increased parallelization can be accomplished by organizing parallel execution accross multiple blocks of multiple threads with the idiomatic expression threadIdx.x + blockIdx.x * blockDim.x. 

### Exercise: Accelerating a For Loop with Multiple Blocks of Threads

Currently, the loop function inside 02-multi-block-loop.cu runs a for loop that will serially print the numbers 0 through 9. Refactor the loop function to be a CUDA kernel which will launch to execute N iterations in parallel. After successfully refactoring, the numbers 0 through 9 should still be printed. For this exercise, as an additional constraint, use an execution configuration that launches at least 2 blocks of threads. Refer to the solution if you get stuck.

```
/*
 * Refactor `loop` to be a CUDA Kernel. 
 * The new kernel should only do the work 
 * of 1 iteration of the original loop.
 */

#include <cstdio>

void loop(int N)
{
  for (int i = 0; i < N; ++i)
  {
    printf("This is iteration number %d\n", i);
  }
}

int main()
{
  /*
   * When refactoring `loop` to launch as a kernel, be sure
   * to use the execution configuration to control how many
   * "iterations" to perform.
   *
   * For this exercise, be sure to use more than 1 block in
   * the execution configuration.
   */

  int N = 10;
  loop(N);
}
```

## Allocating Memory to be accessed on the GPU and the CPU

More recent versions of CUDA (version 6 and later) have made it easy to allocate memory that is available to both the CPU host and any number of GPU devices, and while there are many intermediate and advanced techniques for memory management that will support the most optimal performance in accelerated applications, the most basic CUDA memory management technique we will now cover supports fantastic performance gains over CPU-only applications with almost no developer overhead.

To allocate and free memory, and obtain a pointer that can be referenced in both host and device code, replace calls to malloc and free with cudaMallocManaged and cudaFree as in the following example:

// CPU-only

int N = 2<<20;
size_t size = N * sizeof(int);

int *a;
a = (int *)malloc(size);

// Use `a` in CPU-only program.

free(a);
// Accelerated

int N = 2<<20;
size_t size = N * sizeof(int);

int *a;
// Note the address of `a` is passed as first argument.
cudaMallocManaged(&a, size);

// Use `a` on the CPU and/or on any GPU in the accelerated system.

cudaFree(a);

### Exercise: Array Manipulation on both the Host and Device

The 01-double-elements.cu program allocates an array, initializes it with integer values on the host, attempts to double each of these values in parallel on the GPU, and then confirms whether or not the doubling operations were successful, on the host. Currently the program will not work: it is attempting to interact on both the host and the device with an array at pointer a, but has only allocated the array (using malloc) to be accessible on the host. Refactor the application to meet the following conditions, referring to the solution if you get stuck:

#include <stdio.h>

/*
 * Initialize array values on the host.
 */

void init(int *a, int N)
{
  int i;
  for (i = 0; i < N; ++i)
  {
    a[i] = i;
  }
}

/*
 * Double elements in parallel on the GPU.
 */

__global__
void doubleElements(int *a, int N)
{
  int i;
  i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < N)
  {
    a[i] *= 2;
  }
}

/*
 * Check all elements have been doubled on the host.
 */

bool checkElementsAreDoubled(int *a, int N)
{
  int i;
  for (i = 0; i < N; ++i)
  {
    if (a[i] != i*2) return false;
  }
  return true;
}

int main()
{
  int N = 100;
  int *a;

  size_t size = N * sizeof(int);

  /*
   * Refactor this memory allocation to provide a pointer
   * `a` that can be used on both the host and the device.
   */

  a = (int *)malloc(size);

  init(a, N);

  size_t threads_per_block = 10;
  size_t number_of_blocks = 10;

  /*
   * This launch will not work until the pointer `a` is also
   * available to the device.
   */

  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);
  cudaDeviceSynchronize();

  bool areDoubled = checkElementsAreDoubled(a, N);
  printf("All elements were doubled? %s\n", areDoubled ? "TRUE" : "FALSE");

  /*
   * Refactor to free memory that has been allocated to be
   * accessed by both the host and the device.
   */

  free(a);
}


a should be available to both host and device code.
The memory at a should be correctly freed.

!nvcc -arch=sm_70 -o double-elements 05-allocate/01-double-elements.cu -run

## Grid Size Work Amount Mismatch

The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections.

%%HTML

<div align="center"><iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTn2HX1FyUO94g5TxgBm0C7pu-_5UXPwYtMkhGLnqgs0-2Y1g8CE3YCuZuob25wrrXz0x8cT9_XxyBl/embed?start=false&loop=false&delayms=3000" frameborder="0" width="900" height="550" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe></div>

## Handling Block Configuration Mismatches to Number of Needed Threads

It may be the case that an execution configuration cannot be expressed that will create the exact number of threads needed for parallelizing a loop.

A common example has to do with the desire to choose optimal block sizes. For example, due to GPU hardware traits, blocks that contain a number of threads that are a multiple of 32 are often desirable for performance benefits. Assuming that we wanted to launch blocks each containing 256 threads (a multiple of 32), and needed to run 1000 parallel tasks (a trivially small number for ease of explanation), then there is no number of blocks that would produce an exact total of 1000 threads in the grid, since there is no integer value 32 can be multiplied by to equal exactly 1000.

This scenario can be easily addressed in the following way:

Write an execution configuration that creates more threads than necessary to perform the allotted work.
Pass a value as an argument into the kernel (N) that represents to the total size of the data set to be processed, or the total threads that are needed to complete the work.
After calculating the thread's index within the grid (using tid+bid*bdim), check that this index does not exceed N, and only perform the pertinent work of the kernel if it does not.
Here is an example of an idiomatic way to write an execution configuration when both N and the number of threads in a block are known, and an exact match between the number of threads in the grid and N cannot be guaranteed. It ensures that there are always at least as many threads as needed for N, and only 1 additional block's worth of threads extra, at most:

// Assume `N` is known
int N = 100000;

// Assume we have a desire to set `threads_per_block` exactly to `256`
size_t threads_per_block = 256;

// Ensure there are at least `N` threads in the grid, but only 1 block's worth extra
size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;

some_kernel<<<number_of_blocks, threads_per_block>>>(N);
Becuase the execution configuration above results in more threads in the grid than N, care will need to be taken inside of the some_kernel definition so that some_kernel does not attempt to access out of range data elements, when being executed by one of the "extra" threads:

__global__ some_kernel(int N)
{
  int idx = threadIdx.x + blockIdx.x * blockDim.x;

  if (idx < N) // Check to make sure `idx` maps to some value within `N`
  {
    // Only do work if it does
  }
}

### Exercise: Accelerating a For Loop with a Mismatched Execution Configuration

The program in 02-mismatched-config-loop.cu allocates memory, using cudaMallocManaged for a 1000 element array of integers, and then seeks to initialize all the values of the array in parallel using a CUDA kernel. This program assumes that both N and the number of threads_per_block are known. Your task is to complete the following two objectives, refer to the solution if you get stuck:

Assign a value to number_of_blocks that will make sure there are at least as many threads as there are elements in a to work on.
Update the initializeElementsTo kernel to make sure that it does not attempt to work on data elements that are out of range.

#include <stdio.h>

/*
 * Currently, `initializeElementsTo`, if executed in a thread whose
 * `i` is calculated to be greater than `N`, will try to access a value
 * outside the range of `a`.
 *
 * Refactor the kernel defintition to prevent our of range accesses.
 */

__global__ void initializeElementsTo(int initialValue, int *a, int N)
{
  int i = threadIdx.x + blockIdx.x * blockDim.x;
  a[i] = initialValue;
}

int main()
{
  /*
   * Do not modify `N`.
   */

  int N = 1000;

  int *a;
  size_t size = N * sizeof(int);

  cudaMallocManaged(&a, size);

  /*
   * Assume we have reason to want the number of threads
   * fixed at `256`: do not modify `threads_per_block`.
   */

  size_t threads_per_block = 256;

  /*
   * Assign a value to `number_of_blocks` that will
   * allow for a working execution configuration given
   * the fixed values for `N` and `threads_per_block`.
   */

  size_t number_of_blocks = 0;

  int initialValue = 6;

  initializeElementsTo<<<number_of_blocks, threads_per_block>>>(initialValue, a, N);
  cudaDeviceSynchronize();

  /*
   * Check to make sure all values in `a`, were initialized.
   */

  for (int i = 0; i < N; ++i)
  {
    if(a[i] != initialValue)
    {
      printf("FAILURE: target value: %d\t a[%d]: %d\n", initialValue, i, a[i]);
      exit(1);
    }
  }
  printf("SUCCESS!\n");

  cudaFree(a);
}


!nvcc -arch=sm_70 -o mismatched-config-loop 05-allocate/02-mismatched-config-loop.cu -run

## Grid-Stride Loops

The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections.

%%HTML

<div align="center"><iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTSfcPagyv7ObRnhygFnKrvDIDa-wUuc3yR-qs7xd4gQxProMOqXzNqe8y9vz711cLIbPp1qYJc7R3l/embed?start=false&loop=false&delayms=3000" frameborder="0" width="900" height="550" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe></div>

Data Sets Larger then the Grid
Either by choice, often to create the most performant execution configuration, or out of necessity, the number of threads in a grid may be smaller than the size of a data set. Consider an array with 1000 elements, and a grid with 250 threads (using trivial sizes here for ease of explanation). Here, each thread in the grid will need to be used 4 times. One common method to do this is to use a grid-stride loop within the kernel.

In a grid-stride loop, each thread will calculate its unique index within the grid using tid+bid*bdim, perform its operation on the element at that index within the array, and then, add to its index the number of threads in the grid and repeat, until it is out of range of the array. For example, for a 500 element array and a 250 thread grid, the thread with index 20 in the grid would:

Perform its operation on element 20 of the 500 element array
Increment its index by 250, the size of the grid, resulting in 270
Perform its operation on element 270 of the 500 element array
Increment its index by 250, the size of the grid, resulting in 520
Because 520 is now out of range for the array, the thread will stop its work
CUDA provides a special variable giving the number of blocks in a grid, gridDim.x. Calculating the total number of threads in a grid then is simply the number of blocks in a grid multiplied by the number of threads in each block, gridDim.x * blockDim.x. With this in mind, here is a verbose example of a grid-stride loop within a kernel:

__global void kernel(int *a, int N)
{
  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;
  int gridStride = gridDim.x * blockDim.x;

  for (int i = indexWithinTheGrid; i < N; i += gridStride)
  {
    // do work on a[i];
  }
}

### Exercise: Use a Grid-Stride Loop to Manipulate an Array Larger than the Grid

Refactor 03-grid-stride-double.cu to use a grid-stride loop in the doubleElements kernel, in order that the grid, which is smaller than N, can reuse threads to cover every element in the array. The program will print whether or not every element in the array has been doubled, currently the program accurately prints FALSE. Refer to the solution if you get stuck.

#include <stdio.h>

void init(int *a, int N)
{
  int i;
  for (i = 0; i < N; ++i)
  {
    a[i] = i;
  }
}

/*
 * In the current application, `N` is larger than the grid.
 * Refactor this kernel to use a grid-stride loop in order that
 * each parallel thread work on more than one element of the array.
 */

__global__
void doubleElements(int *a, int N)
{
  int i;
  i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < N)
  {
    a[i] *= 2;
  }
}

bool checkElementsAreDoubled(int *a, int N)
{
  int i;
  for (i = 0; i < N; ++i)
  {
    if (a[i] != i*2) return false;
  }
  return true;
}

int main()
{
  /*
   * `N` is greater than the size of the grid (see below).
   */

  int N = 10000;
  int *a;

  size_t size = N * sizeof(int);
  cudaMallocManaged(&a, size);

  init(a, N);

  /*
   * The size of this grid is 256*32 = 8192.
   */

  size_t threads_per_block = 256;
  size_t number_of_blocks = 32;

  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);
  cudaDeviceSynchronize();

  bool areDoubled = checkElementsAreDoubled(a, N);
  printf("All elements were doubled? %s\n", areDoubled ? "TRUE" : "FALSE");

  cudaFree(a);
}

!nvcc -arch=sm_70 -o grid-stride-double 05-allocate/03-grid-stride-double.cu -run

## Error Handling

As in any application, error handling in accelerated CUDA code is essential. Many, if not most CUDA functions (see, for example, the memory management functions) return a value of type cudaError_t, which can be used to check whether or not an error occured while calling the function. Here is an example where error handling is performed for a call to cudaMallocManaged:

cudaError_t err;
err = cudaMallocManaged(&a, N)                    // Assume the existence of `a` and `N`.

if (err != cudaSuccess)                           // `cudaSuccess` is provided by CUDA.
{
  printf("Error: %s\n", cudaGetErrorString(err)); // `cudaGetErrorString` is provided by CUDA.
}
Launching kernels, which are defined to return void, do not return a value of type cudaError_t. To check for errors occuring at the time of a kernel launch, for example if the launch configuration is erroneous, CUDA provides the cudaGetLastError function, which does return a value of type cudaError_t.

/*
 * This launch should cause an error, but the kernel itself
 * cannot return it.
 */

someKernel<<<1, -1>>>();  // -1 is not a valid number of threads.

cudaError_t err;
err = cudaGetLastError(); // `cudaGetLastError` will return the error from above.
if (err != cudaSuccess)
{
  printf("Error: %s\n", cudaGetErrorString(err));
}
Finally, in order to catch errors that occur asynchronously, for example during the execution of an asynchronous kernel, it is essential to check the status returned by a subsequent synchronizing cuda runtime API call, such as cudaDeviceSynchronize, which will return an error if one of the kernels launched previously should fail.

### Exercise: Add Error Handling

Currently 01-add-error-handling.cu compiles, runs, and prints that the elements of the array were not successfully doubled. The program does not, however, indicate that there are any errors within it. Refactor the application to handle CUDA errors so that you can learn what is wrong with the program and effectively debug it. You will need to investigate both synchronous errors potentially created when calling CUDA functions, as well as asynchronous errors potentially created while a CUDA kernel is executing. Refer to the solution if you get stuck.

#include <stdio.h>

void init(int *a, int N)
{
  int i;
  for (i = 0; i < N; ++i)
  {
    a[i] = i;
  }
}

__global__
void doubleElements(int *a, int N)
{

  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int stride = gridDim.x * blockDim.x;

  for (int i = idx; i < N + stride; i += stride)
  {
    a[i] *= 2;
  }
}

bool checkElementsAreDoubled(int *a, int N)
{
  int i;
  for (i = 0; i < N; ++i)
  {
    if (a[i] != i*2) return false;
  }
  return true;
}

int main()
{
  /*
   * Add error handling to this source code to learn what errors
   * exist, and then correct them. Googling error messages may be
   * of service if actions for resolving them are not clear to you.
   */

  int N = 10000;
  int *a;

  size_t size = N * sizeof(int);
  cudaMallocManaged(&a, size);

  init(a, N);

  size_t threads_per_block = 2048;
  size_t number_of_blocks = 32;

  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);
  cudaDeviceSynchronize();

  bool areDoubled = checkElementsAreDoubled(a, N);
  printf("All elements were doubled? %s\n", areDoubled ? "TRUE" : "FALSE");

  cudaFree(a);
}

!nvcc -arch=sm_70 -o add-error-handling 06-errors/01-add-error-handling.cu -run

## CUDA Error Handling Function

It can be helpful to create a macro that wraps CUDA function calls for checking errors. Here is an example, feel free to use it in the remaining exercises:

#include <stdio.h>
#include <assert.h>

inline cudaError_t checkCuda(cudaError_t result)
{
  if (result != cudaSuccess) {
    fprintf(stderr, "CUDA Runtime Error: %s\n", cudaGetErrorString(result));
    assert(result == cudaSuccess);
  }
  return result;
}

int main()
{

/*
 * The macro can be wrapped around any function returning
 * a value of type `cudaError_t`.
 */

  checkCuda( cudaDeviceSynchronize() )
}

## Summary

At this point in time you have accomplished all of the following lab objectives:

Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.
Control parallel thread hierarchy using execution configuration.
Refactor serial loops to execute their iterations in parallel on a GPU.
Allocate and free memory available to both CPUs and GPUs.
Handle errors generated by CUDA code.
Now you will complete the final objective of the lab:

Accelerate CPU-only applications.
Final Exercise: Accelerate Vector Addition Application
The following challenge will give you an opportunity to use everything that you have learned thus far in the lab. It involves accelerating a CPU-only vector addition program, which, while not the most sophisticated program, will give you an opportunity to focus on what you have learned about GPU-accelerating an application with CUDA. After completing this exercise, if you have time and interest, continue on to the Advanced Content section for some challenges that involve more complex codebases.

01-vector-add.cu contains a functioning CPU-only vector addition application. Accelerate its addVectorsInto function to run as a CUDA kernel on the GPU and to do its work in parallel. Consider the following that need to occur, and refer to the solution if you get stuck.

Augment the addVectorsInto definition so that it is a CUDA kernel.
Choose and utilize a working execution configuration so that addVectorsInto launches as a CUDA kernel.
Update memory allocations, and memory freeing to reflect that the 3 vectors a, b, and result need to be accessed by host and device code.
Refactor the body of addVectorsInto: it will be launched inside of a single thread, and only needs to do one thread's worth of work on the input vectors. Be certain the thread will never try to access elements outside the range of the input vectors, and take care to note whether or not the thread needs to do work on more than one element of the input vectors.
Add error handling in locations where CUDA code might otherwise silently fail.
!nvcc -arch=sm_70 -o vector-add 07-vector-add/01-vector-add.cu -run
Advanced Content
The following exercises provide additional challenge for those with time and interest. They require the use of more advanced techniques, and provide less scaffolding. They are difficult and excellent for your development.

Grids and Blocks of 2 and 3 Dimensions
Grids and blocks can be defined to have up to 3 dimensions. Defining them with multiple dimensions does not impact their performance in any way, but can be very helpful when dealing with data that has multiple dimensions, for example, 2d matrices. To define either grids or blocks with two or 3 dimensions, use CUDA's dim3 type as such:

dim3 threads_per_block(16, 16, 1);
dim3 number_of_blocks(16, 16, 1);
someKernel<<<number_of_blocks, threads_per_block>>>();
Given the example just above, the variables gridDim.x, gridDim.y, blockDim.x, and blockDim.y inside of someKernel, would all be equal to 16.

### Exercise: Accelerate 2D Matrix Multiply Application
The file 01-matrix-multiply-2d.cu contains a host function matrixMulCPU which is fully functional. Your task is to build out the matrixMulGPU CUDA kernel. The source code will execute the matrix multiplication with both functions, and compare their answers to verify the correctness of the CUDA kernel you will be writing. Use the following guidelines to support your work and refer to the solution if you get stuck:

You will need to create an execution configuration whose arguments are both dim3 values with the x and y dimensions set to greater than 1.
Inside the body of the kernel, you will need to establish the running thread's unique index within the grid per usual, but you should establish two indices for the thread: one for the x axis of the grid, and one for the y axis of the grid.
!nvcc -arch=sm_70 -o matrix-multiply-2d 08-matrix-multiply/01-matrix-multiply-2d.cu -run
FOUND ERROR at c[1][0]
### Exercise: Accelerate A Thermal Conductivity Application
In the following exercise, you will be accelerating an application that simulates the thermal conduction of silver in 2 dimensional space.

Convert the step_kernel_mod function inside 01-heat-conduction.cu to execute on the GPU, and modify the main function to properly allocate data for use on CPU and GPU. The step_kernel_ref function executes on the CPU and is used for error checking. Because this code involves floating point calculations, different processors, or even simply reording operations on the same processor, can result in slightly different results. For this reason the error checking code uses an error threshold, instead of looking for an exact match. Refer to the solution if you get stuck.

!nvcc -arch=sm_70 -o heat-conduction 09-heat/01-heat-conduction.cu -run

Credit for the original Heat Conduction CPU source code in this task is given to the article An OpenACC Example Code for a C-based heat conduction code from the University of Houston.
